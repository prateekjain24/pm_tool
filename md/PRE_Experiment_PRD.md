# Product Requirements Document: Pre-Experiment Check Tool

## Executive Summary

### The Challenge

Product teams waste 30-40% of their experiments due to poor planning. Without proper validation, teams run tests that are doomed to fail, wasting weeks of development time and missing opportunities to improve their products.

### The Solution

The Pre-Experiment Check Tool is an intelligent workspace that guides Product Managers through structured hypothesis creation using a conversational form interface. It uses advanced AI to analyze and improve hypotheses, calculates the right sample size with visual tools, and validates experiment designs - all before any development begins.

### Business Impact

- **Save 2-3 weeks** per experiment by catching design flaws early
- **Increase experiment success rate by 25%** through structured hypothesis design
- **Reduce decision-making time from hours to minutes** with automated calculations
- **Build PM expertise** with AI-powered coaching and feedback

### Investment Required

- 3-month development timeline
- 4-person team (Product, Design, 2 Engineers)
- LLM API costs for hypothesis analysis

## Problem Definition

### Who Experiences This Problem?

**Primary Users: Product Managers**

- Struggle to write clear, testable hypotheses
- Unsure about required sample sizes for their experiments
- Waste time on manual calculations and guesswork
- Face pressure to show results but lack scientific rigor

**Secondary Users: Leadership Teams**

- Need confidence in experiment decisions
- Want to optimize resource allocation
- Require clear documentation of experiment rationale

### The Real Cost of Poor Experiment Design

**Time Waste**

- Average failed experiment costs 3 weeks of development time
- PMs spend 2+ hours manually calculating sample sizes
- Teams run experiments for wrong durations (too short or unnecessarily long)

**Business Impact**

- Poor decisions based on inconclusive results
- Missed opportunities to improve key metrics
- Reduced team morale from repeated experiment failures
- Loss of stakeholder trust in the experimentation process

### Why Current Solutions Fall Short

**Manual Calculations**

- Error-prone and time-consuming
- Require statistical knowledge many PMs lack
- Don't provide guidance on hypothesis quality

**Generic Calculators**

- Disconnected from actual business context
- No integration with experiment platforms
- Lack visual feedback and interpretation

## Solution Overview

### What Is the Pre-Experiment Check Tool?

An intelligent workspace where Product Managers can validate their experiment ideas before implementation. Think of it as a "spell-checker" for experiments - it catches mistakes before they become costly failures.

### Core Capabilities

**1. Guided Hypothesis Builder**

- Step-by-step form interface (like Typeform)
- Forces structured thinking with required fields
- AI-powered analysis after submission
- Specific, actionable improvement suggestions

**2. Visual Sample Size Calculator**

- Interactive sliders for key parameters
- Live graphs showing trade-offs
- Plain English explanations
- Preset templates for common scenarios

**3. Experiment Timeline Predictor**

- Calendar view of experiment duration
- Traffic input for duration calculations
- Risk indicators for common pitfalls
- Go/no-go recommendations

**4. Documentation Generator**

- Auto-formatted experiment plans
- Stakeholder-ready summaries (PDF/Markdown)
- One-click export functionality

### What This Tool Won't Do

- Won't run the actual experiments
- Won't integrate with other tools (standalone solution)
- Won't make decisions for you (provides recommendations)
- Won't replace statistical analysis tools (focuses on pre-experiment phase)

## User Journey

### Sarah's Story: From Idea to Validated Experiment

**Step 1: Guided Hypothesis Creation (5 minutes)**
Sarah starts the hypothesis builder and is greeted with a friendly, conversational interface:

_Screen 1_: "What change do you want to test?"

- Sarah types: "Change checkout button color to green"

_Screen 2_: "Who will see this change?"

- Multiple choice options + custom field
- Sarah selects: "Mobile users" and adds "in the US market"

_Screen 3_: "Why do you think this will work?"

- Sarah writes: "Our user research shows green is associated with 'go' and security"

_Screen 4_: "What impact do you expect?"

- Sarah enters: "Increase checkout conversion rate by 10%"

_Screen 5_: "How will you measure success?"

- Checkboxes for common metrics + custom fields
- Sarah selects: "Conversion rate", "Revenue per visitor"

The tool assembles her hypothesis: "We believe that changing the checkout button to green for mobile users in the US market will increase checkout conversion rate by 10% because our user research shows green is associated with 'go' and security. We'll know this is true when we see improvements in conversion rate and revenue per visitor."

**Step 2: AI-Powered Analysis (30 seconds)**
Sarah clicks "Analyze My Hypothesis" and the AI provides feedback:

âœ… **Strengths:**

- Clear intervention specified
- Target audience defined
- Measurable outcome included

ğŸ”§ **Improvements Needed:**

- "Your expected lift of 10% is ambitious. Based on similar color change experiments, 2-5% is more realistic."
- "Consider adding a secondary metric like 'cart abandonment rate' to catch any negative effects."
- "Your reasoning could be stronger. Include specific data: '73% of users in our survey said...'"

**Improved Score: 7/10**

**Step 3: Sample Size Calculation (2 minutes)**
Sarah moves to the calculator with her hypothesis pre-filled:

- Current conversion rate: 3.2% (she enters)
- Minimum detectable effect: 3% (AI-suggested based on feedback)
- Interactive graphs show she needs 25,000 users per variant

**Step 4: Timeline Planning (1 minute)**
Based on her traffic input (5,000 users/day), the tool shows:

- Experiment duration: 10 days
- Calendar view with start/end dates
- Warning: "âš ï¸ Includes weekend - traffic may vary"

**Step 5: Export Documentation (30 seconds)**
Sarah clicks "Generate Experiment Plan" and downloads:

- PDF summary for stakeholders
- Markdown file for documentation
- Hypothesis scorecard with improvement history

### Total Time: 9 minutes (vs. 2+ hours manually)

## Key Features Deep Dive

### 1. Guided Hypothesis Builder with AI Analysis

**Conversational Form Interface**

- Step-by-step questions that build a complete hypothesis
- Each step requires completion before proceeding (no skipping)
- Dynamic paths based on user responses
- Required fields with validation ensure quality inputs
- Progress indicator shows completion status
- Cannot submit for analysis until all steps are complete

**The "We Believe That" Framework**
Every hypothesis follows this structure:

- **We believe that** [intervention]
- **For** [target audience]
- **Will result in** [measurable outcome]
- **Because** [reasoning based on data/insights]
- **We'll know this works when** [success metrics]

**AI-Powered Feedback Engine**
Using advanced language models to provide:

- Contextual suggestions based on your industry
- Realistic lift expectations from similar experiments
- Missing element identification
- Risk warnings (e.g., "This might affect page load time")
- Alternative hypothesis suggestions
- Use Websearch & Context 7 to get the latest SDK details for LLMs
- Have support for OpenAI o3-mini, Claude Sonnet 4 & Google gemini 2.5 pro

**Scoring Methodology**
Hypotheses are scored on five dimensions:

1. **Clarity** (Is the change specific and clear?)
2. **Measurability** (Can we track the outcome?)
3. **Reasoning** (Is there evidence supporting this?)
4. **Scope** (Is it focused on one change?)
5. **Testability** (Can we run this experiment?)

### 2. Interactive Sample Size Calculator

**Visual Elements**

```
[Current Rate: 5%] â”€â”€sliderâ”€â”€> [Desired Lift: 10%] â”€â”€sliderâ”€â”€> [Sample Size: 8,500]
                                        |
                                        v
                              [Duration: 14 days]
```

**Interactive Visualization Graphs**

- **Power Curve**: Updates as you adjust sliders to show relationship between sample size and detection ability
- **Duration Timeline**: Dynamically calculates experiment length based on traffic input
- **MDE Trade-off Graph**: Shows how sample size changes with different minimum detectable effects
- **Traffic Impact Visualization**: Illustrates how daily traffic affects experiment duration

**Smart Features**

- Preset scenarios (e.g., "High-stakes revenue test", "Quick iteration test")
- Traffic forecasting based on historical patterns
- Automatic adjustments for multiple variants
- Statistical power indicator

### 3. Experiment Timeline Predictor

**Calendar Visualization**

- Gantt-chart style view showing experiment duration
- Manual input for expected traffic patterns
- Built-in calendar of common retail/holiday periods
- Visual timeline with key milestones

**Smart Alerts Based on Dates**

- "âš ï¸ This includes Black Friday - expect traffic spikes"
- "âœ“ No major holidays during this period"
- "ğŸ“Š Allow extra days for weekend traffic dips"

### 4. One-Click Documentation

**Output Formats**

- Executive Summary (1-page PDF)
- Technical Specification (for developers)
- Experiment Plan (detailed methodology)
- Quick Share Link (for Slack/email)

**Export Options**

- PDF summaries for stakeholder meetings
- Markdown files for documentation
- CSV data for further analysis
- Shareable links (view-only)

## Business Value & ROI

### Quantifiable Benefits

**Time Savings**

- Reduce experiment planning from 2 hours to 8 minutes
- Save 2-3 weeks per failed experiment avoided
- **Annual time saved per PM: 150+ hours**

**Success Rate Improvement**

- Current experiment success rate: 35%
- Projected success rate: 60%
- **Value of additional successful experiments: $2M+ annually**

**Resource Optimization**

- Avoid running underpowered experiments
- Prevent over-investment in simple tests
- **Engineering time saved: 20%**

### Strategic Benefits

**Data-Driven Culture**

- Standardizes experimentation practices
- Builds PM confidence in statistics
- Creates shared language across teams

**Competitive Advantage**

- Faster iteration cycles
- Higher quality product decisions
- More predictable development timelines

## Success Metrics

### Primary Metrics

**Adoption & Usage**

- Target: 80% of PMs using tool weekly within 6 months
- Measurement: Platform analytics

**Experiment Quality**

- Target: 25% increase in experiment success rate
- Measurement: Post-experiment surveys and success tracking

**Time Efficiency**

- Target: 90% reduction in experiment planning time
- Measurement: User surveys + time tracking

### Secondary Metrics

**User Satisfaction**

- Target: 4.5+ star rating
- Measurement: In-app feedback

**Hypothesis Quality Score**

- Target: Average score improvement from 4 to 8
- Measurement: Tool analytics

**Documentation Completeness**

- Target: 95% of experiments have full documentation
- Measurement: Export tracking and usage analytics

### Leading Indicators

- Number of hypotheses checked per week
- Percentage of experiments using recommended sample sizes
- Repeat usage rate
- Feature adoption funnel

## Launch Strategy

### Phase 1: Beta Launch (Month 1)

**Target Users**: 10 volunteer PMs from different teams

**Goals**:

- Validate core workflows
- Gather feedback on AI analysis quality
- Test export functionality

**Success Criteria**:

- 8/10 PMs want to continue using
- Average time savings of 75%+

### Phase 2: Team Rollout (Month 2-3)

**Target**: 3 full product teams

**Approach**:

- Team training sessions
- Dedicated Slack channel
- Weekly office hours

**Enhancements**:

- Custom scoring for different product areas
- Team templates and presets

### Phase 3: Company-Wide Launch (Month 4+)

**Rollout Strategy**:

- Executive sponsor announcement
- Integration into PM onboarding
- Success story showcase

**Support Plan**:

- Video tutorials library
- Best practices documentation
- Peer mentorship program

## Risk Assessment & Mitigation

### Adoption Risks

**Risk**: PMs resist changing their workflow

- **Mitigation**: Start with volunteers, showcase time savings
- **Mitigation**: Make tool optional initially, let success stories drive adoption

**Risk**: Statistical complexity intimidates users

- **Mitigation**: Use progressive disclosure (basic â†’ advanced)
- **Mitigation**: Provide "explain like I'm five" mode

### Technical Risks

**Risk**: AI provides incorrect recommendations

- **Mitigation**: Human-in-the-loop validation
- **Mitigation**: Clear disclaimers and confidence scores
- **Mitigation**: Regular model updates based on feedback

**Risk**: LLM API reliability and costs

- **Mitigation**: Implement caching for common patterns
- **Mitigation**: Fallback to rule-based suggestions if API fails

### Business Risks

**Risk**: Over-reliance on tool recommendations

- **Mitigation**: Emphasize tool as assistant, not decision-maker
- **Mitigation**: Require human approval for high-stakes experiments

## Appendix: Visual Mockups

### Guided Hypothesis Builder Interface

**Step 1: What Change?**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 1 of 5                    â—‹â—â—‹â—‹â—‹   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                         â”‚
â”‚  What change do you want to test? ğŸ”¬   â”‚
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Describe your change...          â”‚  â”‚
â”‚  â”‚                                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                         â”‚
â”‚  ğŸ’¡ Example: "Add a progress bar to   â”‚
â”‚     the checkout flow"                  â”‚
â”‚                                         â”‚
â”‚        [Back]     [Continue â†’]          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Step 5: AI Analysis Results**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Your Hypothesis Analysis       Score: 7/10  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                         â”‚
â”‚  ğŸ“ Your Hypothesis:                    â”‚
â”‚  "We believe that adding a progress bar â”‚
â”‚  to checkout for mobile users will      â”‚
â”‚  increase completion rate by 10%..."    â”‚
â”‚                                         â”‚
â”‚  âœ… Strengths:                         â”‚
â”‚  â€¢ Clear, specific change              â”‚
â”‚  â€¢ Defined target audience             â”‚
â”‚  â€¢ Measurable outcome                  â”‚
â”‚                                         â”‚
â”‚  ğŸ”§ Recommendations:                    â”‚
â”‚  â€¢ "10% lift is optimistic. Similar    â”‚
â”‚    experiments show 3-5% improvement"   â”‚
â”‚  â€¢ "Add 'time to complete checkout'    â”‚
â”‚    as a secondary metric"              â”‚
â”‚  â€¢ "Include data: '67% of users        â”‚
â”‚    abandoned at step 3'"               â”‚
â”‚                                         â”‚
â”‚  [Revise] [Accept & Continue]          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Sample Size Calculator

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Sample Size Calculator             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Your Hypothesis: Progress bar checkout â”‚
â”‚  AI-Suggested MDE: 3% (was 10%)       â”‚
â”‚                                         â”‚
â”‚  ğŸ“Š Interactive Controls:               â”‚
â”‚  Baseline Rate:    [3.2%]â”€â”€â”€â—â”€â”€â”€â”€      â”‚
â”‚  Min. Detectable:  [3%]â”€â”€â”€â”€â—â”€â”€â”€â”€â”€      â”‚
â”‚  Confidence:       [95%]â”€â”€â”€â”€â—â”€â”€â”€â”€      â”‚
â”‚  Power:           [80%]â”€â”€â”€â”€â—â”€â”€â”€â”€â”€      â”‚
â”‚                                         â”‚
â”‚  ğŸ“ˆ Visual Results:                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚     Sample Size vs MDE Graph     â”‚   â”‚
â”‚  â”‚         Shows curve with         â”‚   â”‚
â”‚  â”‚      current position marked     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                         â”‚
â”‚  ğŸ“Š You need: 25,000 users/variant     â”‚
â”‚  â±ï¸  Duration: 10 days at 5k users/day â”‚
â”‚                                         â”‚
â”‚  ğŸ’¡ Pro tip: Reducing MDE to 2% would  â”‚
â”‚     require 56,000 users (22 days)     â”‚
â”‚                                         â”‚
â”‚  [Download Report] [Continue â†’]         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Timeline Visualization

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Experiment Timeline            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Nov 1   Nov 8   Nov 15   Nov 22       â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€>     â”‚
â”‚  â”‚       â”‚       â”‚        â”‚            â”‚
â”‚  Start   25%     50%      End          â”‚
â”‚          â†“       â†“        â†“            â”‚
â”‚        Check   Review   Results        â”‚
â”‚                                         â”‚
â”‚  âš ï¸ Nov 23-25: Thanksgiving traffic dip â”‚
â”‚  âœ“ Completion before monthly review     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Conclusion

The Pre-Experiment Check Tool transforms experiment design from a time-consuming, error-prone process into a quick, confident decision. By catching problems before they become expensive failures, we'll help Product Managers run better experiments, make better decisions, and ultimately build better products.

**Next Steps**:

1. Approve PRD and allocate resources
2. Begin design sprint for UI/UX
3. Set up beta program recruitment
4. Select LLM provider and negotiate pricing
